{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Dataset Metadata\n",
    "# Determine Subjects and Node Positions\n",
    "ultrasound_positions = ['1TCC', '1TCC1', '2TMLI', '3TANI']\n",
    "subject_names = ['Sub011', 'Sub012', 'Sub013', 'Sub014', 'Sub015', 'Sub016', 'Sub017']\n",
    "num_of_classes = 6\n",
    "\n",
    "# Output dataset path\n",
    "dataset_path = 'datasets'\n",
    "\n",
    "# Test Subject (Leave one out)\n",
    "test_subjects = ['Sub011']\n",
    "\n",
    "# Classes Needs to be dropped\n",
    "to_drop = [1, 2]\n",
    "\n",
    "# Reset\n",
    "reset = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train images\n",
      "All files deleted successfully.\n",
      "train labels\n",
      "All files deleted successfully.\n",
      "val images\n",
      "All files deleted successfully.\n",
      "val labels\n",
      "All files deleted successfully.\n",
      "test images\n",
      "All files deleted successfully.\n",
      "test labels\n",
      "All files deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def delete_files_in_directory(directory_path):\n",
    "    files = os.listdir(directory_path)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "    print(\"All files deleted successfully.\")\n",
    "\n",
    "if reset:\n",
    "  folders = ['train', 'val', 'test']\n",
    "  sub_folders = ['images', 'labels']\n",
    "  directory_path = 'datasets/'\n",
    "  for folder in folders:\n",
    "    for sub_folder in sub_folders:\n",
    "      print(folder, sub_folder)\n",
    "      delete_files_in_directory(directory_path + folder + '/' + sub_folder + '/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub011 has 39 empty annotations out of 252 annotated files in 1TCC position\n",
      "Sub011 has 11 empty annotations out of 252 annotated files in 1TCC1 position\n",
      "Sub012 has 110 empty annotations out of 252 annotated files in 1TCC position\n",
      "Sub012 has 53 empty annotations out of 126 annotated files in 1TCC1 position\n",
      "Sub013 has 13 empty annotations out of 252 annotated files in 1TCC position\n",
      "Sub014 has 156 empty annotations out of 252 annotated files in 1TCC position\n",
      "Sub015 has 64 empty annotations out of 253 annotated files in 1TCC position\n",
      "Sub015 has 43 empty annotations out of 126 annotated files in 1TCC1 position\n",
      "Sub016 has 69 empty annotations out of 393 annotated files in 1TCC position\n",
      "Sub016 has 10 empty annotations out of 131 annotated files in 1TCC1 position\n",
      "Sub016 has 1 empty annotations out of 124 annotated files in 3TANI position\n",
      "Sub017 has 144 empty annotations out of 252 annotated files in 1TCC position\n",
      "Sub017 has 60 empty annotations out of 126 annotated files in 1TCC1 position\n",
      "==============================\n",
      "Total 773 empty annotations found out of 4686 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "filenames = []\n",
    "total_empty_count = 0\n",
    "total_file_count = 0\n",
    "\n",
    "for subject in subject_names:\n",
    "    # print(f\"{subject}\")\n",
    "    for position in ultrasound_positions:\n",
    "        # print(f\"{position}\")\n",
    "        # Variables\n",
    "        empty_count = 0\n",
    "        annotation_count = 0\n",
    "\n",
    "        bounding_box_filenames = []\n",
    "        # Parameters\n",
    "        mypath = Path(f\"Dataset/{subject}/Trachea/{position}/Reviewed/\")\n",
    "        pattern = r\".*\\.txt$\"\n",
    "\n",
    "        for filename in os.listdir(mypath):\n",
    "            if re.search(pattern, filename):\n",
    "                bounding_box_filenames.append(filename)\n",
    "        annotation_count = len(bounding_box_filenames)\n",
    "        total_file_count += annotation_count\n",
    "        for filename in bounding_box_filenames:\n",
    "            with open(mypath / filename, 'r') as file_obj:\n",
    "                first_char = file_obj.read(1)\n",
    "                if not first_char:\n",
    "                    # print(f\"{filename} is empty\")\n",
    "                    empty_count += 1\n",
    "                else:\n",
    "                    filenames.append(f\"{subject}_{position}_{str(filename[:-4])}\")\n",
    "                # # Count number of images\n",
    "                # annotations = file_obj.readlines()\n",
    "                # for annotation in annotations:\n",
    "                #     class_label, x1, y1, x2, y2 = annotation.strip().split()\n",
    "                #     class_count[class_label] += 1\n",
    "# Results\n",
    "        if empty_count > 0:\n",
    "            print(f\"{subject} has {empty_count} empty annotations out of {annotation_count} annotated files in {position} position\")\n",
    "            total_empty_count += empty_count\n",
    "print(\"==============================\")\n",
    "print(f\"Total {total_empty_count} empty annotations found out of {total_file_count} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sub011_1TCC_F000047_frame123'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Train Test Split\n",
    "def train_test(data, test_size, seed):\n",
    "    random.seed(seed)\n",
    "    X = data\n",
    "    random.shuffle(X)\n",
    "    print(len(X))\n",
    "    split_idx = int(test_size * len(X))\n",
    "    print(split_idx)\n",
    "    return X[split_idx:], X[:split_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "# check if there are overlapping names\n",
    "temp = [item for item, count in collections.Counter(filenames).items() if count > 1]\n",
    "print(temp)\n",
    "print(len(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Train-Val-Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total non-empty files 3913\n",
      "After creating test files 3446\n",
      "3446\n",
      "344\n",
      "3102 344\n",
      "train images 3102\n",
      "val images 344\n",
      "test images 467\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "print(f'Total non-empty files {len(filenames)}')\n",
    "# Create Test Set (leave one out)\n",
    "for subject in test_subjects:\n",
    "    # print(f\"{subject}\")\n",
    "    for position in ultrasound_positions:\n",
    "        # print(f\"{position}\")\n",
    "        mypath = Path(f\"Dataset/{subject}/Trachea/{position}/Reviewed/\")\n",
    "        pattern = r\".*\\.txt$\"\n",
    "\n",
    "        bounding_box_filenames_again = []\n",
    "\n",
    "        for filename in os.listdir(mypath):\n",
    "            if re.search(pattern, filename):\n",
    "                bounding_box_filenames_again.append(filename[:-4])\n",
    "\n",
    "        for filename in bounding_box_filenames_again:\n",
    "            new_filename = f\"{subject}_{position}_{str(filename)}\"\n",
    "            # print(new_filename)\n",
    "            if new_filename not in filenames:\n",
    "                continue\n",
    "            shutil.copy(str(mypath / filename) + '.png', f'{dataset_path}/test/images/{new_filename}.png')\n",
    "            shutil.copy(str(mypath / filename) + '.txt', f'{dataset_path}/test/labels/{new_filename}.txt')\n",
    "            filenames.remove(new_filename)\n",
    "\n",
    "print(f'After creating test files {len(filenames)}')\n",
    "# Create Train-Val sets\n",
    "train_list, val_list = train_test(filenames, test_size=0.1, seed=44)\n",
    "print(len(train_list), len(val_list))\n",
    "for subject in subject_names:\n",
    "    if subject in test_subjects:\n",
    "        continue\n",
    "    # print(f\"{subject}\")\n",
    "    for position in ultrasound_positions:\n",
    "        # print(f\"{position}\")\n",
    "        mypath = Path(f\"Dataset/{subject}/Trachea/{position}/Reviewed/\")\n",
    "        pattern = r\".*\\.txt$\"\n",
    "\n",
    "        bounding_box_filenames_again = []\n",
    "\n",
    "        for filename in os.listdir(mypath):\n",
    "            if re.search(pattern, filename):\n",
    "                bounding_box_filenames_again.append(filename[:-4])\n",
    "        \n",
    "        for filename in bounding_box_filenames_again:\n",
    "            new_filename = f\"{subject}_{position}_{str(filename)}\"\n",
    "            if new_filename not in filenames:\n",
    "                continue\n",
    "            if new_filename in train_list:\n",
    "                shutil.copy(f'Dataset/{subject}/Trachea/{position}/Reviewed/{filename}.png', f'{dataset_path}/train/images/{new_filename}.png')\n",
    "                shutil.copy(f'Dataset/{subject}/Trachea/{position}/Reviewed/{filename}.txt', f'{dataset_path}/train/labels/{new_filename}.txt')\n",
    "            elif new_filename in val_list:\n",
    "                shutil.copy(f'Dataset/{subject}/Trachea/{position}/Reviewed/{filename}.png', f'{dataset_path}/val/images/{new_filename}.png')\n",
    "                shutil.copy(f'Dataset/{subject}/Trachea/{position}/Reviewed/{filename}.txt', f'{dataset_path}/val/labels/{new_filename}.txt')\n",
    "\n",
    "# Output\n",
    "temp_path = Path(f'datasets/train/images/')\n",
    "print(f'train images {len(os.listdir(temp_path))}')\n",
    "temp_path = Path(f'datasets/val/images/')\n",
    "print(f'val images {len(os.listdir(temp_path))}')\n",
    "temp_path = Path(f'datasets/test/images/')\n",
    "print(f'test images {len(os.listdir(temp_path))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "folders = ['train', 'val', 'test']\n",
    "sub_folders = ['labels']\n",
    "\n",
    "# create updated classes list to substitute dropped classes\n",
    "to_drop.sort()\n",
    "j = 0\n",
    "value_map = {}\n",
    "for i in range(num_of_classes-1, 0, -1):\n",
    "    if j == len(to_drop):\n",
    "        break\n",
    "    if i not in to_drop and to_drop[j] < i:\n",
    "        value_map[i] = to_drop[j]\n",
    "        j += 1\n",
    "    \n",
    "\n",
    "for folder in folders:\n",
    "  for sub_folder in sub_folders:\n",
    "    mypath = Path(f\"{dataset_path}/{folder}/{sub_folder}/\")\n",
    "\n",
    "    for filename in os.listdir(mypath):\n",
    "      # read file\n",
    "      df_file = pd.read_csv(mypath / filename, delimiter=\" \", names=['class', 'x', 'y', 'w', 'h'])\n",
    "\n",
    "      # bounding box correction\n",
    "      df_file.loc[df_file['w'] > 1, 'w'] = 1.0\n",
    "      df_file.loc[df_file['w'] < 0, 'w'] = 0.0\n",
    "      df_file.loc[df_file['h'] > 1, 'h'] = 1.0\n",
    "      df_file.loc[df_file['h'] < 0, 'h'] = 0.0\n",
    "\n",
    "      # drop classes\n",
    "      df_file = df_file[~df_file['class'].isin(to_drop)]\n",
    "      for key, value in value_map.items():\n",
    "        df_file.loc[df_file['class'] == key, 'class'] = value\n",
    "      \n",
    "      # write file\n",
    "      df_file.to_csv(mypath / filename, index=False, header=False, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
